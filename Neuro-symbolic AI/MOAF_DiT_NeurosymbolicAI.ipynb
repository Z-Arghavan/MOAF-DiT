{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "2SydYQMwMEot"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai rdflib faiss-cpu pandas tqdm google-generativeai\n",
        "# faiss-cpu(fast vector similarity search for GraphRAG retrieval)\n",
        "# tqdm (progress bars for long operations (e.g. embedding))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7c2099b"
      },
      "source": [
        "### Using Google Gemini instead of OpenAPI\n",
        "\n",
        "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio. In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`.\n",
        "\n",
        "Then, we'll configure the Gemini API and define a new embedding function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87468ad6"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key from Colab secrets\n",
        "GOOGLE_API_KEY=userdata.get('MOAFDiT2')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "c1f53415",
        "outputId": "03f1b9c0-6fe6-48c3-d1ae-0d8f0e406602"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Embedding model used for semantic retrieval from Gemini\n",
        "GEMINI_EMBED_MODEL = \"models/text-embedding-004\" # You can choose another suitable Gemini embedding model if needed\n",
        "\n",
        "def embed_texts_gemini(texts, batch_size=64):\n",
        "    \"\"\"\n",
        "    Convert text into embedding vectors using Google Gemini.\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        chunk = texts[i:i+batch_size]\n",
        "        try:\n",
        "            resp = genai.embed_content(\n",
        "                model=GEMINI_EMBED_MODEL,\n",
        "                content=chunk\n",
        "            )\n",
        "            # Gemini's embed_content returns a dict with 'embedding' key for the list of embeddings\n",
        "            vectors.extend(resp['embedding'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding chunk: {e}\")\n",
        "            # Handle error or skip this chunk\n",
        "            pass # Or raise the exception if you want to stop on error\n",
        "    return np.array(vectors, dtype=\"float32\")\n",
        "\n",
        "# Generate embeddings using the Gemini API\n",
        "embeddings = embed_texts_gemini(entity_cards)\n",
        "print(\"Embeddings shape:\", embeddings.shape)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (150, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97c77938",
        "outputId": "8937f2bb-6a24-48ef-d38f-d61cd4de2063"
      },
      "source": [
        "markdown_content = \"\"\"### Using Google Gemini instead of OpenAPI\n",
        "\n",
        "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.\n",
        "\n",
        "**Important Setup Steps:**\n",
        "1. **Obtain your API Key**: Go to [Google AI Studio](https://aistudio.google.com/app/apikey) and create an API key.\n",
        "2. **Store in Colab Secrets**: In your Colab notebook, click on the \"ðŸ”‘\" icon (Secrets) in the left panel. Add a new secret named `GOOGLE_API_KEY` and paste your API key as its value. Make sure to enable \"Notebook access\" for this secret.\n",
        "\n",
        "Then, we'll configure the Gemini API and define a new embedding function.\n",
        "\"\"\"\n",
        "\n",
        "print(markdown_content)\n",
        "print(\"\\nNote: This code prints the markdown content as a Python string. To actually update cell 'c7c2099b' as a markdown cell, you would need to manually copy this content into that cell and set its type to Markdown.\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Using Google Gemini instead of OpenAPI\n",
            "\n",
            "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.\n",
            "\n",
            "**Important Setup Steps:**\n",
            "1. **Obtain your API Key**: Go to [Google AI Studio](https://aistudio.google.com/app/apikey) and create an API key.\n",
            "2. **Store in Colab Secrets**: In your Colab notebook, click on the \"ðŸ”‘\" icon (Secrets) in the left panel. Add a new secret named `GOOGLE_API_KEY` and paste your API key as its value. Make sure to enable \"Notebook access\" for this secret.\n",
            "\n",
            "Then, we'll configure the Gemini API and define a new embedding function.\n",
            "\n",
            "\n",
            "Note: This code prints the markdown content as a Python string. To actually update cell 'c7c2099b' as a markdown cell, you would need to manually copy this content into that cell and set its type to Markdown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # This opens a file picker so you can upload your ontology.\n",
        "# # Supported formats include Turtle (.ttl), RDF/XML (.rdf, .owl), N-Triples (.nt).\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # We assume you upload exactly one ontology file\n",
        "# ontology_path = next(iter(uploaded.keys()))\n",
        "# ontology_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "D17AnjhoMni2",
        "outputId": "0c0c4475-e5e2-42d0-db3c-304678dc9a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12022e75-8baa-40d7-bf1f-f7b5995b1979\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-12022e75-8baa-40d7-bf1f-f7b5995b1979\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MOAFDITO.rdf to MOAFDITO (1).rdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MOAFDITO (1).rdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "ONTOLOGY_CANDIDATES = [\n",
        "    \"https://z-arghavan.github.io/MOAF-DiT/ontology.ttl\",\n",
        "    \"https://z-arghavan.github.io/MOAF-DiT/ontology.rdf\",\n",
        "    \"https://z-arghavan.github.io/MOAF-DiT/ontology.nt\",\n",
        "    \"https://z-arghavan.github.io/MOAF-DiT/ontology.jsonld\",\n",
        "]\n",
        "\n",
        "def load_remote_ontology() -> Graph:\n",
        "    g = Graph()\n",
        "    last_err = None\n",
        "\n",
        "    for url in ONTOLOGY_CANDIDATES:\n",
        "        try:\n",
        "            g.parse(url)  # rdflib will often infer the format\n",
        "            print(\"Loaded ontology from:\", url)\n",
        "            return g\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "\n",
        "    raise RuntimeError(f\"Could not load ontology from any candidate URL. Last error: {last_err}\")\n",
        "\n",
        "g = load_remote_ontology()\n",
        "print(\"Triples:\", len(g))\n"
      ],
      "metadata": {
        "id": "RA0r2cYY4K-Z",
        "outputId": "e3cf0455-7b94-4620-cfd9-ec0c74202571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded ontology from: https://z-arghavan.github.io/MOAF-DiT/ontology.ttl\n",
            "Triples: 484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# SUPPORTED_EXTENSIONS = (\".ttl\", \".rdf\", \".owl\", \".nt\")\n",
        "\n",
        "# files = [\n",
        "#     f for f in os.listdir(\".\")\n",
        "#     if f.lower().endswith(SUPPORTED_EXTENSIONS)\n",
        "# ]\n",
        "\n",
        "# if not files:\n",
        "#     raise FileNotFoundError(\"No ontology file found in the current environment.\")\n",
        "\n",
        "# ontology_path = files[0]\n",
        "# print(\"Using ontology file:\", ontology_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Y4ItbbwFnd",
        "outputId": "5c2a722f-35d7-4af7-fe7a-bd6adae2c291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ontology file: MOAFDITO.rdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to convert graph nodes (IRIs) into short texts that the LLM can reason over. This is crucial for GraphRAG: LLMs do not understand raw IRIs well. We turn graph structure into textual surrogates\n",
        "\n",
        "from rdflib.namespace import RDF, RDFS, OWL, SKOS\n",
        "from collections import defaultdict\n",
        "\n",
        "# Properties commonly used for labels and descriptions in ontologies\n",
        "LABEL_PROPS = [RDFS.label, SKOS.prefLabel, SKOS.altLabel]\n",
        "DESC_PROPS = [RDFS.comment, SKOS.definition]\n",
        "\n",
        "#Extracting human-readable labels and comments. Fetches labels like rdfs:label, skos:prefLabel. Falls back to compact IRI names if labels are missing. LLMs work on text, not IRIs. This bridges semantic web data and language models.\n",
        "def first_literal(graph, subj, props):\n",
        "    \"\"\"\n",
        "    Return the first literal value for a subject and a list of properties.\n",
        "    Used to extract labels or comments.\n",
        "    \"\"\"\n",
        "    for p in props:\n",
        "        for o in graph.objects(subj, p):\n",
        "            return str(o)\n",
        "    return None\n",
        "\n",
        "def all_literals(graph, subj, props, limit=10):\n",
        "    \"\"\"\n",
        "    Collect up to 'limit' literal values for description properties.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for p in props:\n",
        "        for o in graph.objects(subj, p):\n",
        "            out.append(str(o))\n",
        "            if len(out) >= limit:\n",
        "                return out\n",
        "    return out\n",
        "\n",
        "def compact_iri(iri: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a human-readable short name from an IRI.\n",
        "    This is only for display, not for logic.\n",
        "    \"\"\"\n",
        "    s = str(iri)\n",
        "    if \"#\" in s:\n",
        "        return s.split(\"#\")[-1]\n",
        "    return s.rstrip(\"/\").split(\"/\")[-1]\n"
      ],
      "metadata": {
        "id": "FJuCQ_prNCSQ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "# Load the ontology into a graph object\n",
        "ontology_path = \"https://z-arghavan.github.io/MOAF-DiT/ontology.ttl\"\n",
        "g = Graph()\n",
        "g.parse(ontology_path)\n",
        "\n",
        "# We collect entities that are meaningful in an ontology:\n",
        "# classes, properties, and individuals.\n",
        "\n",
        "entities = set()\n",
        "\n",
        "for s in g.subjects(RDF.type, OWL.Class):\n",
        "    entities.add(s)\n",
        "\n",
        "for s in g.subjects(RDF.type, RDF.Property):\n",
        "    entities.add(s)\n",
        "\n",
        "for s in g.subjects(RDF.type, OWL.ObjectProperty):\n",
        "    entities.add(s)\n",
        "\n",
        "for s in g.subjects(RDF.type, OWL.DatatypeProperty):\n",
        "    entities.add(s)\n",
        "\n",
        "for s in g.subjects(RDF.type, OWL.NamedIndividual):\n",
        "    entities.add(s)\n",
        "\n",
        "# Fallback: anything that has a label\n",
        "for s in g.subjects(RDFS.label, None):\n",
        "    entities.add(s)\n",
        "\n",
        "print(\"Number of candidate entities:\", len(entities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMJx7lN2NmbF",
        "outputId": "4672e4bd-3577-4aba-b7db-a748b13def6a"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of candidate entities: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_types(graph, s, limit=6):\n",
        "    \"\"\"\n",
        "    Get RDF types of an entity, truncated to a small number.\n",
        "    \"\"\"\n",
        "    types = []\n",
        "    for t in graph.objects(s, RDF.type):\n",
        "        types.append(compact_iri(t))\n",
        "        if len(types) >= limit:\n",
        "            break\n",
        "    return types\n",
        "\n",
        "def get_neighbour_triples(graph, s, max_triples=25):\n",
        "    \"\"\"\n",
        "    Retrieve a small local neighbourhood of triples\n",
        "    around an entity (incoming and outgoing).\n",
        "    \"\"\"\n",
        "    triples = []\n",
        "\n",
        "    # Outgoing triples\n",
        "    for p, o in graph.predicate_objects(s):\n",
        "        triples.append((s, p, o))\n",
        "        if len(triples) >= max_triples:\n",
        "            return triples\n",
        "\n",
        "    # Incoming triples\n",
        "    for subj, pred in graph.subject_predicates(s):\n",
        "        triples.append((subj, pred, s))\n",
        "        if len(triples) >= max_triples:\n",
        "            return triples\n",
        "\n",
        "    return triples\n",
        "\n",
        "def make_entity_card(graph, s):\n",
        "    \"\"\"\n",
        "    Create a short textual 'entity card' for an IRI.\n",
        "    This is what we embed and retrieve in GraphRAG.\n",
        "    \"\"\"\n",
        "    label = first_literal(graph, s, LABEL_PROPS) or compact_iri(s)\n",
        "    descs = all_literals(graph, s, DESC_PROPS, limit=3)\n",
        "    types = get_types(graph, s)\n",
        "\n",
        "    neigh = []\n",
        "    for (ss, pp, oo) in get_neighbour_triples(graph, s, max_triples=18):\n",
        "        ss_txt = first_literal(graph, ss, LABEL_PROPS) or compact_iri(ss)\n",
        "        pp_txt = compact_iri(pp)\n",
        "        oo_txt = (\n",
        "            first_literal(graph, oo, LABEL_PROPS)\n",
        "            if str(oo).startswith(\"http\")\n",
        "            else str(oo)\n",
        "        ) or compact_iri(oo)\n",
        "        neigh.append(f\"{ss_txt} {pp_txt} {oo_txt}\")\n",
        "\n",
        "    lines = [\n",
        "        f\"IRI: {str(s)}\",\n",
        "        f\"Label: {label}\"\n",
        "    ]\n",
        "\n",
        "    if types:\n",
        "        lines.append(\"Types: \" + \", \".join(types))\n",
        "    if descs:\n",
        "        lines.append(\"Notes: \" + \" | \".join(descs))\n",
        "    if neigh:\n",
        "        lines.append(\"Neighbour facts:\")\n",
        "        lines.extend([f\"- {x}\" for x in neigh[:12]])\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "x9wBAC9_NrXm"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate all entity cards\n",
        "#This is where the ontology becomes LLM-readable knowledge.\n",
        "entity_cards = []\n",
        "entity_iris = []\n",
        "\n",
        "for e in entities:\n",
        "    try:\n",
        "        card = make_entity_card(g, e)\n",
        "        if card and len(card) > 30:\n",
        "            entity_cards.append(card)\n",
        "            entity_iris.append(str(e))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"Entity cards created:\", len(entity_cards))\n",
        "print(entity_cards[0][:600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Xbx3acRgsa",
        "outputId": "78600ab2-91e1-42c0-c8a5-97c2e08449f0",
        "collapsed": true
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity cards created: 150\n",
            "IRI: http://www.semanticweb.org/MOAFDITO#hasName\n",
            "Label: has Name\n",
            "Types: DatatypeProperty\n",
            "Neighbour facts:\n",
            "- has Name type DatatypeProperty\n",
            "- has Name domain Department\n",
            "- has Name range string\n",
            "- has Name label has Name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a FAISS vector index (retrieval layer)\n",
        "import faiss\n",
        "\n",
        "# Dimensionality of embeddings\n",
        "dim = embeddings.shape[1]\n",
        "\n",
        "# Inner product index (used as cosine similarity after normalisation)\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "# Normalise vectors so inner product equals cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# Add all entity embeddings to the index\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"FAISS index size:\", index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvEBRJiUVsaW",
        "outputId": "2e315fc0-2b76-4ae6-9f68-cd523ae169ad"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index size: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79531127"
      },
      "source": [
        "#### What does 'FAISS index size' mean?\n",
        "\n",
        "After extracting all meaningful entities (classes, properties, and individuals) from your uploaded ontology, we create a textual representation for each of them, called an 'entity card'. These entity cards are then converted into numerical vectors (embeddings) using a language model.\n",
        "\n",
        "The **FAISS index size** refers to the number of these entity embeddings that have been added to the FAISS vector database. Essentially, it tells you how many unique entities from your ontology are now indexed and searchable by semantic similarity. When you see 'FAISS index size: 162', it means that 162 unique entities from your ontology have been vectorized and stored in the index."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve relevant entities for a question\n",
        "def retrieve_entities(question, top_k=8):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant ontology entities for a natural language question.\n",
        "    \"\"\"\n",
        "    # Use the Gemini embedding function\n",
        "    q_emb = embed_texts_gemini([question])\n",
        "\n",
        "    q_vec = np.array(q_emb, dtype=\"float32\")\n",
        "    faiss.normalize_L2(q_vec)\n",
        "\n",
        "    scores, idxs = index.search(q_vec, top_k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        results.append({\n",
        "            \"score\": float(score),\n",
        "            \"iri\": entity_iris[idx],\n",
        "            \"card\": entity_cards[idx]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "question = \"how can we know if a board is in which process?\"\n",
        "hits = retrieve_entities(question, top_k=6)\n",
        "\n",
        "for h in hits:\n",
        "    print(h[\"score\"], h[\"iri\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "8FdrQ191WM1t",
        "outputId": "3dae6cd1-4090-4f80-dd2d-31f8f961015c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5565840005874634 http://www.semanticweb.org/MOAFDITO#BoardType\n",
            "0.536327064037323 http://www.semanticweb.org/MOAFDITO#hasBoardID\n",
            "0.5309848785400391 http://www.semanticweb.org/MOAFDITO#processedInDepartment\n",
            "0.5277382135391235 http://www.semanticweb.org/MOAFDITO#belongsToBoardFamily\n",
            "0.5109294652938843 http://www.semanticweb.org/MOAFDITO#BoardFamily\n",
            "0.5048397779464722 http://www.semanticweb.org/MOAFDITO#refersToProcessStep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Expand a local subgraph\n",
        "from rdflib import URIRef\n",
        "\n",
        "def triple_to_text(ss, pp, oo):\n",
        "    \"\"\"\n",
        "    Convert a triple into a short readable sentence.\n",
        "    \"\"\"\n",
        "    ss_txt = first_literal(g, ss, LABEL_PROPS) or compact_iri(ss)\n",
        "    pp_txt = compact_iri(pp)\n",
        "    oo_txt = (\n",
        "        first_literal(g, oo, LABEL_PROPS)\n",
        "        if str(oo).startswith(\"http\")\n",
        "        else str(oo)\n",
        "    ) or compact_iri(oo)\n",
        "    return f\"{ss_txt} {pp_txt} {oo_txt}\"\n",
        "\n",
        "def expand_subgraph(seed_iris, max_triples_per_seed=30):\n",
        "    \"\"\"\n",
        "    Retrieve a small neighbourhood of triples\n",
        "    around an entity (incoming and outgoing).\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    out = []\n",
        "\n",
        "    for iri in seed_iris:\n",
        "        s = URIRef(iri)\n",
        "\n",
        "        for p, o in g.predicate_objects(s):\n",
        "            t = (str(s), str(p), str(o))\n",
        "            if t not in seen:\n",
        "                seen.add(t)\n",
        "                out.append((s, p, o))\n",
        "\n",
        "        for subj, pred in g.subject_predicates(s):\n",
        "            t = (str(subj), str(pred), str(s))\n",
        "            if t not in seen:\n",
        "                seen.add(t)\n",
        "                out.append((subj, pred, s))\n",
        "\n",
        "    return out\n",
        "\n",
        "seed_iris = [h[\"iri\"] for h in hits]\n",
        "triples = expand_subgraph(seed_iris)\n",
        "\n",
        "triple_texts = [\n",
        "    f\"T{i+1}: {triple_to_text(ss, pp, oo)}\"\n",
        "    for i, (ss, pp, oo) in enumerate(triples[:120])\n",
        "]\n",
        "\n",
        "print(triple_texts[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtNyg-glXQH-",
        "outputId": "0dc5b626-ae21-4255-bcd8-bb62b98980b2"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T1: Board Type type Class', 'T2: Board Type subClassOf nac560eac68294fb0929240f169686ef7b8', 'T3: Board Type comment Should we create 37 boards here?Update: according to the new report, there are 65 unique board types', 'T4: Board Type label Board Type', 'T5: belongs To Board Family domain Board Type', 'T6: has Board Type range Board Type', 'T7: has Route domain Board Type', 'T8: has Work Order For Board Type range Board Type', 'T9: processed In Department domain Board Type', 'T10: nac560eac68294fb0929240f169686ef7b5 first Board Type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "69e8520e",
        "outputId": "e094b0a1-40d7-454d-ba56-f86b0b255a08",
        "collapsed": true
      },
      "source": [
        "# Define your question here\n",
        "question = input(\"Please enter your question: \")\n",
        "\n",
        "print(f\"Question set: {question}\")"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2267349811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define your question here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter your question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question set: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ask the LLM using GraphRAG\n",
        "import google.generativeai as genai # Ensure genai is imported if not already\n",
        "\n",
        "MODEL = \"models/gemini-2.5-flash\" # Using models/gemini-2.0-flash as requested\n",
        "\n",
        "def answer_with_graphrag(question, hits, triple_texts, max_triples=80):\n",
        "    \"\"\"\n",
        "    Ask the LLM to answer using only the retrieved graph evidence.\n",
        "    \"\"\"\n",
        "    entity_list = \"\\n\".join(\n",
        "        [f\"- {h['iri']} (score={h['score']:.3f})\" for h in hits]\n",
        "    )\n",
        "\n",
        "    context = [\n",
        "        \"Resolved entities:\",\n",
        "        entity_list,\n",
        "        \"\",\n",
        "        \"Knowledge graph evidence (triples):\",\n",
        "    ]\n",
        "    context.extend(triple_texts[:max_triples])\n",
        "\n",
        "    prompt = (\n",
        "        \"Answer the question based only on the provided context.\\n\"\n",
        "        \"If the answer is not available in the context, state that explicitly.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Context:\\n\"\n",
        "        + \"\\n\".join(context)\n",
        "    )\n",
        "\n",
        "    model = genai.GenerativeModel(MODEL)\n",
        "    resp = model.generate_content(prompt)\n",
        "\n",
        "    return resp.text\n",
        "\n",
        "# We have 'question', 'hits', and 'triple_texts' from previous steps\n",
        "# question = input(\"Please enter your question: \") # Use the 'question' variable already set\n",
        "answer = answer_with_graphrag(question, hits, triple_texts)\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "kTlUmoOtXcix",
        "outputId": "a45b0af8-af27-4e2f-c735-332c7fc0f8d4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is departmen\n",
            "\n",
            "Answer: Based on the provided context:\n",
            "\n",
            "The term \"Department\" appears as the range of the `processed In Department` property (T21: `processed In Department range Department`). This means that a `BoardType` (which is the domain of `processed In Department` as per T9) can be processed *in* a `Department`.\n",
            "\n",
            "However, the context does not provide a direct definition, comment, or type (e.g., whether it is a Class, DatatypeProperty, etc.) specifically for \"Department\" itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subgraph expansion end here.**"
      ],
      "metadata": {
        "id": "bYRlz6UYn-KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOW, the code below helps to ask questions in natural language and turn them into SPARQL queries**"
      ],
      "metadata": {
        "id": "TMXDogkwo_2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract a small schema hint for the prompt\n",
        "def schema_hint(graph, max_items=80):\n",
        "    \"\"\"\n",
        "    Build a compact hint about predicates and classes present in the graph.\n",
        "    This helps the LLM write valid SPARQL for your ontology.\n",
        "    \"\"\"\n",
        "    preds = set()\n",
        "    for s, p, o in graph:\n",
        "        preds.add(p)\n",
        "        if len(preds) >= max_items:\n",
        "            break\n",
        "\n",
        "    pred_lines = [f\"- {str(p)}\" for p in list(preds)[:max_items]]\n",
        "    return \"Common predicates you can use:\\n\" + \"\\n\".join(pred_lines)\n",
        "\n",
        "SCHEMA = schema_hint(g, max_items=60)\n",
        "print(SCHEMA[:1200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10UA5MJnoIXu",
        "outputId": "8b46cfa5-0819-4ac7-fecb-5834837c2f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common predicates you can use:\n",
            "- http://www.w3.org/2002/07/owl#onProperty\n",
            "- http://www.w3.org/2002/07/owl#minQualifiedCardinality\n",
            "- http://www.w3.org/2000/01/rdf-schema#subClassOf\n",
            "- http://www.w3.org/2000/01/rdf-schema#range\n",
            "- http://www.w3.org/2000/01/rdf-schema#seeAlso\n",
            "- http://www.w3.org/2000/01/rdf-schema#label\n",
            "- http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
            "- http://www.w3.org/1999/02/22-rdf-syntax-ns#rest\n",
            "- http://www.w3.org/2002/07/owl#onClass\n",
            "- http://purl.org/dc/terms/description\n",
            "- http://www.w3.org/2000/01/rdf-schema#subPropertyOf\n",
            "- http://www.w3.org/2002/07/owl#unionOf\n",
            "- http://www.w3.org/2000/01/rdf-schema#comment\n",
            "- http://www.w3.org/2002/07/owl#inverseOf\n",
            "- http://purl.org/dc/terms/creator\n",
            "- http://www.w3.org/1999/02/22-rdf-syntax-ns#first\n",
            "- http://purl.org/dc/terms/descriptions\n",
            "- http://www.w3.org/2000/01/rdf-schema#domain\n",
            "- http://www.w3.org/2002/07/owl#versionInfo\n",
            "- http://purl.org/dc/terms/contributor\n",
            "- http://purl.org/dc/terms/title\n",
            "- http://purl.org/dc/terms/license\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a vocabulary index from the uploaded ontology\n",
        "import re\n",
        "from rdflib import RDF, RDFS, OWL, URIRef, Literal\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab_index(g):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      vocab: dict with keys \"classes\", \"obj_props\", \"data_props\", \"props\"\n",
        "      label_index: list of (iri, label_text, kind)\n",
        "    \"\"\"\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)) | set(g.subjects(RDF.type, RDFS.Class))\n",
        "    obj_props = set(g.subjects(RDF.type, OWL.ObjectProperty))\n",
        "    data_props = set(g.subjects(RDF.type, OWL.DatatypeProperty))\n",
        "    props = set(g.subjects(RDF.type, RDF.Property)) | obj_props | data_props\n",
        "\n",
        "    def texts(s):\n",
        "        out = []\n",
        "        for p in (RDFS.label, RDFS.comment):\n",
        "            for o in g.objects(s, p):\n",
        "                if isinstance(o, Literal):\n",
        "                    out.append(str(o))\n",
        "        return out\n",
        "\n",
        "    vocab = {\n",
        "        \"classes\": sorted(map(str, classes)),\n",
        "        \"obj_props\": sorted(map(str, obj_props)),\n",
        "        \"data_props\": sorted(map(str, data_props)),\n",
        "        \"props\": sorted(map(str, props)),\n",
        "    }\n",
        "\n",
        "    label_index = []\n",
        "    def add_kind(iris, kind):\n",
        "        for iri in iris:\n",
        "            t = \" \".join(texts(URIRef(iri))).strip()\n",
        "            if t:\n",
        "                label_index.append((iri, t.lower(), kind))\n",
        "            else:\n",
        "                label_index.append((iri, iri.lower(), kind))\n",
        "\n",
        "    add_kind(vocab[\"classes\"], \"class\")\n",
        "    add_kind(vocab[\"obj_props\"], \"object_property\")\n",
        "    add_kind(vocab[\"data_props\"], \"datatype_property\")\n",
        "    add_kind(vocab[\"props\"], \"property\")\n",
        "\n",
        "    return vocab, label_index\n",
        "\n",
        "vocab, label_index = build_vocab_index(g)\n",
        "len(vocab[\"classes\"]), len(vocab[\"props\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nesdrXx0x9kn",
        "outputId": "8eb41519-81e3-4f89-f01f-ad812e91229c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve only the terms that look relevant to the question\n",
        "import math\n",
        "\n",
        "def normalise(text: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
        "\n",
        "def retrieve_terms(question: str, label_index, top_k=40):\n",
        "    q = normalise(question)\n",
        "    q_tokens = set(q.split())\n",
        "\n",
        "    scored = []\n",
        "    for iri, label_text, kind in label_index:\n",
        "        lt = normalise(label_text)\n",
        "        lt_tokens = set(lt.split())\n",
        "        overlap = len(q_tokens & lt_tokens)\n",
        "\n",
        "        if overlap == 0:\n",
        "            continue\n",
        "\n",
        "        # light bonus if direct substring match exists\n",
        "        bonus = 2 if any(tok in lt for tok in q_tokens) else 0\n",
        "        score = overlap + bonus\n",
        "        scored.append((score, iri, kind, label_text))\n",
        "\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "    return scored[:top_k]\n",
        "\n",
        "def format_allow_list(term_hits):\n",
        "    lines = []\n",
        "    for score, iri, kind, label_text in term_hits:\n",
        "        snippet = label_text[:160].replace(\"\\n\", \" \")\n",
        "        lines.append(f\"- <{iri}>  ({kind})  label_or_desc: {snippet}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "term_hits = retrieve_terms(\"List the properties and definitions for the concept Deconstruction, if it exists.\", label_index)\n",
        "print(format_allow_list(term_hits[:15]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0luYPWmyO-3",
        "outputId": "88a93325-170c-40fc-92a4-7062f5cb3dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- <http://www.semanticweb.org/MOAFDITO#Material_preparation_personnel>  (class)  label_or_desc: personnel who checks the materials required for assembly and delivers them to the line workers.\n",
            "- <http://www.semanticweb.org/MOAFDITO#Wave_soldering_exit_personnel>  (class)  label_or_desc: aybar has inspector entity. should this person be inspector or not? the personnel who visually inspects the boards exiting the soldering machine and intervenes \n",
            "- <http://www.semanticweb.org/MOAFDITO#processedInDepartment>  (object_property)  label_or_desc: processed in department a bit explanatin: if a boardtype hasroute some route, and that route routehasstep some step, and that step indepartment some department,\n",
            "- <http://www.semanticweb.org/MOAFDITO#processedInDepartment>  (property)  label_or_desc: processed in department a bit explanatin: if a boardtype hasroute some route, and that route routehasstep some step, and that step indepartment some department,\n",
            "- <http://www.semanticweb.org/MOAFDITO#BoardMaterialRequirement>  (class)  label_or_desc: maybe the name can be changed. but in essence, we need a class to say for which board type we need what materials.\n",
            "- <http://www.semanticweb.org/MOAFDITO#Wave_soldering_entry_personnel>  (class)  label_or_desc: the person who sets up the machine and places the boards into the masking pallet.\n",
            "- <http://www.semanticweb.org/MOAFDITO#BoardType>  (class)  label_or_desc: board type should we create 37 boards here?update: according to the new report, there are 65 unique board types\n",
            "- <http://www.semanticweb.org/MOAFDITO#Camera>  (class)  label_or_desc: the system that performs visual quality control using a camera.\n",
            "- <http://www.semanticweb.org/MOAFDITO#Operator>  (class)  label_or_desc: personnel who operate manual machines and perform resistor shaping processes.\n",
            "- <http://www.semanticweb.org/MOAFDITO#Responsible_technician>  (class)  label_or_desc: the technical personnel who delivers barcode labels to the first line worker.\n",
            "- <http://www.semanticweb.org/MOAFDITO#Supervisor>  (class)  label_or_desc: supervisor people who manage people and processes\n",
            "- <http://www.semanticweb.org/MOAFDITO#forWIPBoard>  (object_property)  label_or_desc: for wip board\n",
            "- <http://www.semanticweb.org/MOAFDITO#hasWorkOrderForBoardFamily>  (object_property)  label_or_desc: has work order for board family\n",
            "- <http://www.semanticweb.org/MOAFDITO#hasWorkOrderForBoardType>  (object_property)  label_or_desc: has work order for board type\n",
            "- <http://www.semanticweb.org/MOAFDITO#forWIPBoard>  (property)  label_or_desc: for wip board\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993f05ce",
        "outputId": "598aa481-920a-45f3-c697-a76a752cc089",
        "collapsed": true
      },
      "source": [
        " # Define your question for SPARQL here\n",
        "sparql_question = input(\"Please enter your question for SPARQL generation: \")\n",
        "\n",
        "# print(f\"SPARQL Question set: {sparql_question}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your question for SPARQL generation: What changes in route selection could reduce load in Testing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "15603381",
        "outputId": "b7205e04-2b99-4152-f7fb-87abf6103297"
      },
      "source": [
        "#Gemini generates SPARQL\n",
        "def question_to_sparql(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Use Gemini to write a SPARQL query for the given question.\n",
        "    We instruct it to only output SPARQL, no explanations. Use the uploaded ontology only.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(MODEL)\n",
        "    term_hits = retrieve_terms(question, label_index, top_k=50)\n",
        "    allow_list = format_allow_list(term_hits)\n",
        "\n",
        "    prompt = (\n",
        "        \"You write SPARQL for an RDF graph.\\n\"\n",
        "        \"Return only SPARQL, no prose.\\n\"\n",
        "        \"Closed world constraint:\\n\"\n",
        "        \"1) You MUST use only IRIs that appear in the allow list below and the available ontologies online.\\n\"\n",
        "        \"2) If the allow list is insufficient, write a query that discovers candidates using rdfs:label and rdfs:comment within this graph.\\n\"\n",
        "        \"3) Use LIMIT 50.\\n\\n\"\n",
        "        \"Allow list of IRIs (use only these):\\n\"\n",
        "        f\"{allow_list}\\n\\n\"\n",
        "        \"Question:\\n\"\n",
        "        f\"{question}\\n\"\n",
        "        f\"{SCHEMA}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = model.generate_content(prompt)\n",
        "    sparql_query = resp.text.strip()\n",
        "\n",
        "    # Remove markdown code block fences if present\n",
        "    if sparql_query.startswith('```sparql'):\n",
        "        sparql_query = sparql_query.lstrip('```sparql').rstrip('```').strip()\n",
        "    elif sparql_query.startswith('```'):\n",
        "        sparql_query = sparql_query.lstrip('```').rstrip('```').strip()\n",
        "\n",
        "    return sparql_query\n",
        "\n",
        "def run_sparql(query: str):\n",
        "    \"\"\"\n",
        "    Execute SPARQL on the local rdflib graph.\n",
        "    \"\"\"\n",
        "    return g.query(query)\n",
        "\n",
        "# q2 = \"List the properties and definitions for the concept Deconstruction, if it exists.\" # Old hardcoded question\n",
        "sparql = question_to_sparql(sparql_question) # Using the new input variable\n",
        "print(sparql)\n",
        "\n",
        "results = run_sparql(sparql)\n",
        "print(\"Rows:\", len(results))\n",
        "for row in list(results)[:10]:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX moafdito: <http://www.semanticweb.org/MOAFDITO#>\n",
            "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
            "\n",
            "SELECT DISTINCT ?entity ?route ?testingStep\n",
            "WHERE {\n",
            "  # Find route steps that are identified as Testing or EOL Testing\n",
            "  ?testingStep rdf:type ?testingType .\n",
            "  FILTER (?testingType = moafdito:Testing || ?testingType = moafdito:EOL_Testing) .\n",
            "\n",
            "  # Find the route that contains this specific testing step\n",
            "  ?route moafdito:hasRouteStep ?testingStep .\n",
            "\n",
            "  # Find entities (e.g., BoardMaterialRequirements or other items) that are associated with this route\n",
            "  ?entity moafdito:hasRoute ?route .\n",
            "}\n",
            "LIMIT 50\n",
            "Rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#validator that rejects hallucinated IRIs, then auto repairz\n",
        "from rdflib.plugins.sparql.parser import parseQuery\n",
        "\n",
        "IRI_RE = re.compile(r\"<([^>]+)>\")\n",
        "\n",
        "def extract_iris_from_sparql(q: str):\n",
        "    return set(IRI_RE.findall(q))\n",
        "\n",
        "def iri_exists_in_graph(g, iri: str) -> bool:\n",
        "    u = URIRef(iri)\n",
        "    return (u, None, None) in g or (None, u, None) in g or (None, None, u) in g\n",
        "\n",
        "def validate_sparql_against_graph(g, q: str):\n",
        "    # syntax check\n",
        "    try:\n",
        "        parseQuery(q)\n",
        "    except Exception as e:\n",
        "        return False, f\"SPARQL parse error: {e}\"\n",
        "\n",
        "    # vocabulary check\n",
        "    iris = extract_iris_from_sparql(q)\n",
        "    unknown = [iri for iri in iris if not iri_exists_in_graph(g, iri)]\n",
        "    if unknown:\n",
        "        return False, \"Unknown IRIs not found in graph:\\n\" + \"\\n\".join(f\"- {u}\" for u in sorted(set(unknown)))\n",
        "    return True, \"OK\"\n",
        "\n",
        "def repair_sparql(question: str, bad_query: str, error_msg: str) -> str:\n",
        "    model = genai.GenerativeModel(MODEL)\n",
        "\n",
        "    term_hits = retrieve_terms(question, label_index, top_k=80)\n",
        "    allow_list = format_allow_list(term_hits)\n",
        "\n",
        "    prompt = (\n",
        "        \"Fix the SPARQL query.\\n\"\n",
        "        \"Return only SPARQL, no prose.\\n\"\n",
        "        \"You MUST use only IRIs from the allow list.\\n\\n\"\n",
        "        f\"Error:\\n{error_msg}\\n\\n\"\n",
        "        f\"Bad query:\\n{bad_query}\\n\\n\"\n",
        "        \"Allow list:\\n\"\n",
        "        f\"{allow_list}\\n\\n\"\n",
        "        f\"Question:\\n{question}\\n\"\n",
        "        \"Constraints: LIMIT 50.\\n\"\n",
        "    )\n",
        "    resp = model.generate_content(prompt)\n",
        "    q = resp.text.strip()\n",
        "    if q.startswith(\"```\"):\n",
        "        q = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", q)\n",
        "        q = re.sub(r\"\\s*```$\", \"\", q).strip()\n",
        "    return q\n",
        "\n",
        "def question_to_valid_sparql(question: str, max_fixes: int = 2) -> str:\n",
        "    q = question_to_sparql(question)\n",
        "    ok, msg = validate_sparql_against_graph(g, q)\n",
        "    fixes = 0\n",
        "    while not ok and fixes < max_fixes:\n",
        "        q = repair_sparql(question, q, msg)\n",
        "        ok, msg = validate_sparql_against_graph(g, q)\n",
        "        fixes += 1\n",
        "    return q\n"
      ],
      "metadata": {
        "id": "UTIBe2dNxFiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}